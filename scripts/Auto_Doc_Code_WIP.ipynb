{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "f0d07891",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extract and list the following details from the provided abstract syntax tree:\n",
      "- **Source Tables** (table names)\n",
      "- **Source Columns** (column names)\n",
      "- **Target Tables** (table names)\n",
      "- **Target Columns** (column names)\n",
      "- **Transformations** (describe operations applied)\n",
      "\n",
      "Code:\n",
      "from pyspark.sql import SparkSession\n",
      "\n",
      "def main():\n",
      "    # Start Spark session\n",
      "    spark = SparkSession.builder.appName(\"SQL_ETL_with_Variable\").getOrCreate()\n",
      "\n",
      "    # -----------------------------\n",
      "    # 1. EXTRACT\n",
      "    # -----------------------------\n",
      "    source_table = \"source_db.people\"\n",
      "    df = spark.sql(f\"SELECT id, name, age FROM {source_table}\")\n",
      "    print(\"Original Data:\")\n",
      "    df.show()\n",
      "\n",
      "    # -----------------------------\n",
      "    # 2. TRANSFORM using SQL query\n",
      "    # -----------------------------\n",
      "    # Register temp view\n",
      "    df.createOrReplaceTempView(\"people_view\")\n",
      "\n",
      "    # SQL query as a variable\n",
      "    sql_query = \"\"\"\n",
      "        SELECT \n",
      "            id,\n",
      "            name,\n",
      "            age,\n",
      "            CASE \n",
      "                WHEN age > 30 THEN 'senior'\n",
      "                ELSE 'junior'\n",
      "            END AS status\n",
      "        FROM people_view\n",
      "        WHERE age IS NOT NULL\n",
      "    \"\"\"\n",
      "\n",
      "    transformed_df = spark.sql(sql_query)\n",
      "    print(\"Transformed Data:\")\n",
      "    transformed_df.show()\n",
      "\n",
      "    # -----------------------------\n",
      "    # 3. LOAD\n",
      "    # -----------------------------\n",
      "    target_table = \"target_db.people_status\"\n",
      "    transformed_df.write.mode(\"overwrite\").saveAsTable(target_table)\n",
      "\n",
      "    print(f\"Data successfully written to {target_table}\")\n",
      "\n",
      "    spark.stop()\n",
      "\n",
      "\n",
      "if __name__ == \"__main__\":\n",
      "    main()\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "def extract_code_from_notebook(notebook_path):\n",
    "    \"\"\"Extracts and concatenates code cells from a Jupyter notebook.\"\"\"\n",
    "    with open(notebook_path, 'r', encoding='utf-8') as f:\n",
    "        nb = json.load(f)\n",
    "\n",
    "    code_cells = [\n",
    "        cell['source']\n",
    "        for cell in nb.get('cells', [])\n",
    "        if cell['cell_type'] == 'code'\n",
    "    ]\n",
    "\n",
    "    # Flatten the source lines and join them into a single string\n",
    "    code = \"\\n\".join(\"\".join(cell) for cell in code_cells)\n",
    "    return code\n",
    "\n",
    "# Example usage\n",
    "notebook_path = Path(r\"C:\\Users\\samle\\OneDrive\\Documents\\Utils\\Notebook_Test.ipynb\")\n",
    "code_string = extract_code_from_notebook(notebook_path)\n",
    "\n",
    "\n",
    "prompt = f\"\"\"Extract and list the following details from the provided abstract syntax tree:\n",
    "- **Source Tables** (table names)\n",
    "- **Source Columns** (column names)\n",
    "- **Target Tables** (table names)\n",
    "- **Target Columns** (column names)\n",
    "- **Transformations** (describe operations applied)\n",
    "\n",
    "Code:\n",
    "{code_string}\n",
    "\"\"\"\n",
    "\n",
    "print(prompt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "2d057961",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of tokens: 422\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "# Load the tokenizer for CodeT5\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"Salesforce/codet5-base-multi-sum\")\n",
    "\n",
    "\n",
    "# Tokenize the text and count the tokens\n",
    "tokens = tokenizer.tokenize(prompt)\n",
    "num_tokens = len(tokens)\n",
    "\n",
    "print(f\"Number of tokens: {num_tokens}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b64865e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AST for Code Cell 1:\n",
      "Module(\n",
      "    body=[\n",
      "        ImportFrom(\n",
      "            module='pyspark.sql',\n",
      "            names=[\n",
      "                alias(name='SparkSession')],\n",
      "            level=0),\n",
      "        FunctionDef(\n",
      "            name='main',\n",
      "            args=arguments(\n",
      "                posonlyargs=[],\n",
      "                args=[],\n",
      "                kwonlyargs=[],\n",
      "                kw_defaults=[],\n",
      "                defaults=[]),\n",
      "            body=[\n",
      "                Assign(\n",
      "                    targets=[\n",
      "                        Name(id='spark', ctx=Store())],\n",
      "                    value=Call(\n",
      "                        func=Attribute(\n",
      "                            value=Call(\n",
      "                                func=Attribute(\n",
      "                                    value=Attribute(\n",
      "                                        value=Name(id='SparkSession', ctx=Load()),\n",
      "                                        attr='builder',\n",
      "                                        ctx=Load()),\n",
      "                                    attr='appName',\n",
      "                                    ctx=Load()),\n",
      "                                args=[\n",
      "                                    Constant(value='SQL_ETL_with_Variable')],\n",
      "                                keywords=[]),\n",
      "                            attr='getOrCreate',\n",
      "                            ctx=Load()),\n",
      "                        args=[],\n",
      "                        keywords=[])),\n",
      "                Assign(\n",
      "                    targets=[\n",
      "                        Name(id='source_table', ctx=Store())],\n",
      "                    value=Constant(value='source_db.people')),\n",
      "                Assign(\n",
      "                    targets=[\n",
      "                        Name(id='df', ctx=Store())],\n",
      "                    value=Call(\n",
      "                        func=Attribute(\n",
      "                            value=Name(id='spark', ctx=Load()),\n",
      "                            attr='sql',\n",
      "                            ctx=Load()),\n",
      "                        args=[\n",
      "                            JoinedStr(\n",
      "                                values=[\n",
      "                                    Constant(value='SELECT id, name, age FROM '),\n",
      "                                    FormattedValue(\n",
      "                                        value=Name(id='source_table', ctx=Load()),\n",
      "                                        conversion=-1)])],\n",
      "                        keywords=[])),\n",
      "                Expr(\n",
      "                    value=Call(\n",
      "                        func=Name(id='print', ctx=Load()),\n",
      "                        args=[\n",
      "                            Constant(value='Original Data:')],\n",
      "                        keywords=[])),\n",
      "                Expr(\n",
      "                    value=Call(\n",
      "                        func=Attribute(\n",
      "                            value=Name(id='df', ctx=Load()),\n",
      "                            attr='show',\n",
      "                            ctx=Load()),\n",
      "                        args=[],\n",
      "                        keywords=[])),\n",
      "                Expr(\n",
      "                    value=Call(\n",
      "                        func=Attribute(\n",
      "                            value=Name(id='df', ctx=Load()),\n",
      "                            attr='createOrReplaceTempView',\n",
      "                            ctx=Load()),\n",
      "                        args=[\n",
      "                            Constant(value='people_view')],\n",
      "                        keywords=[])),\n",
      "                Assign(\n",
      "                    targets=[\n",
      "                        Name(id='sql_query', ctx=Store())],\n",
      "                    value=Constant(value=\"\\n\\n        SELECT \\n\\n            id,\\n\\n            name,\\n\\n            age,\\n\\n            CASE \\n\\n                WHEN age > 30 THEN 'senior'\\n\\n                ELSE 'junior'\\n\\n            END AS status\\n\\n        FROM people_view\\n\\n        WHERE age IS NOT NULL\\n\\n    \")),\n",
      "                Assign(\n",
      "                    targets=[\n",
      "                        Name(id='transformed_df', ctx=Store())],\n",
      "                    value=Call(\n",
      "                        func=Attribute(\n",
      "                            value=Name(id='spark', ctx=Load()),\n",
      "                            attr='sql',\n",
      "                            ctx=Load()),\n",
      "                        args=[\n",
      "                            Name(id='sql_query', ctx=Load())],\n",
      "                        keywords=[])),\n",
      "                Expr(\n",
      "                    value=Call(\n",
      "                        func=Name(id='print', ctx=Load()),\n",
      "                        args=[\n",
      "                            Constant(value='Transformed Data:')],\n",
      "                        keywords=[])),\n",
      "                Expr(\n",
      "                    value=Call(\n",
      "                        func=Attribute(\n",
      "                            value=Name(id='transformed_df', ctx=Load()),\n",
      "                            attr='show',\n",
      "                            ctx=Load()),\n",
      "                        args=[],\n",
      "                        keywords=[])),\n",
      "                Assign(\n",
      "                    targets=[\n",
      "                        Name(id='target_table', ctx=Store())],\n",
      "                    value=Constant(value='target_db.people_status')),\n",
      "                Expr(\n",
      "                    value=Call(\n",
      "                        func=Attribute(\n",
      "                            value=Call(\n",
      "                                func=Attribute(\n",
      "                                    value=Attribute(\n",
      "                                        value=Name(id='transformed_df', ctx=Load()),\n",
      "                                        attr='write',\n",
      "                                        ctx=Load()),\n",
      "                                    attr='mode',\n",
      "                                    ctx=Load()),\n",
      "                                args=[\n",
      "                                    Constant(value='overwrite')],\n",
      "                                keywords=[]),\n",
      "                            attr='saveAsTable',\n",
      "                            ctx=Load()),\n",
      "                        args=[\n",
      "                            Name(id='target_table', ctx=Load())],\n",
      "                        keywords=[])),\n",
      "                Expr(\n",
      "                    value=Call(\n",
      "                        func=Name(id='print', ctx=Load()),\n",
      "                        args=[\n",
      "                            JoinedStr(\n",
      "                                values=[\n",
      "                                    Constant(value='Data successfully written to '),\n",
      "                                    FormattedValue(\n",
      "                                        value=Name(id='target_table', ctx=Load()),\n",
      "                                        conversion=-1)])],\n",
      "                        keywords=[])),\n",
      "                Expr(\n",
      "                    value=Call(\n",
      "                        func=Attribute(\n",
      "                            value=Name(id='spark', ctx=Load()),\n",
      "                            attr='stop',\n",
      "                            ctx=Load()),\n",
      "                        args=[],\n",
      "                        keywords=[]))],\n",
      "            decorator_list=[])],\n",
      "    type_ignores=[])\n",
      "AST for Code Cell 2:\n",
      "Module(\n",
      "    body=[\n",
      "        If(\n",
      "            test=Compare(\n",
      "                left=Name(id='__name__', ctx=Load()),\n",
      "                ops=[\n",
      "                    Eq()],\n",
      "                comparators=[\n",
      "                    Constant(value='__main__')]),\n",
      "            body=[\n",
      "                Expr(\n",
      "                    value=Call(\n",
      "                        func=Name(id='main', ctx=Load()),\n",
      "                        args=[],\n",
      "                        keywords=[]))],\n",
      "            orelse=[])],\n",
      "    type_ignores=[])\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import ast\n",
    "\n",
    "notebook_path = r\"C:\\Users\\samle\\OneDrive\\Documents\\Utils\\Notebook_Test.ipynb\"\n",
    "\n",
    "# Load the Jupyter Notebook\n",
    "with open(notebook_path, \"r\") as f:\n",
    "    notebook = json.load(f)\n",
    "\n",
    "# Extract code cells\n",
    "code_cells = [cell['source'] for cell in notebook['cells'] if cell['cell_type'] == 'code']\n",
    "\n",
    "# Parse each code cell into an AST\n",
    "for i, code in enumerate(code_cells):\n",
    "    try:\n",
    "        tree = ast.parse(\"\\n\".join(code))\n",
    "        print(f\"AST for Code Cell {i + 1}:\")\n",
    "        print(ast.dump(tree, indent=4))  # Pretty-print the AST\n",
    "    except SyntaxError as e:\n",
    "        print(f\"Syntax Error in Code Cell {i + 1}: {e}\")\n",
    "\n",
    "prompt = f\"\"\"Extract and list the following details from the provided abstract syntax tree:\n",
    "- **Source Tables** (table names)\n",
    "- **Source Columns** (column names)\n",
    "- **Target Tables** (table names)\n",
    "- **Target Columns** (column names)\n",
    "- **Transformations** (describe operations applied)\n",
    "\n",
    "Code:\n",
    "{tree}\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "ae1b6925",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "6496d6d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.6.0+cpu\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31405279",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "3eafb350",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import math\n",
    "\n",
    "def chunk_code(code_string, chunk_size=500, overlap=100):\n",
    "    \"\"\"Splits the code into chunks with overlap to preserve context.\"\"\"\n",
    "    tokens = re.split(r'(\\s+)', code_string)  # Split by whitespace\n",
    "    total_tokens = len(tokens)\n",
    "    chunks = []\n",
    "\n",
    "    for start in range(0, total_tokens, chunk_size - overlap):\n",
    "        end = min(start + chunk_size, total_tokens)\n",
    "        chunk = \"\".join(tokens[start:end])\n",
    "        chunks.append(chunk)\n",
    "    \n",
    "    return chunks\n",
    "\n",
    "\n",
    "\n",
    "chunks = chunk_code(code_string, chunk_size=50, overlap=100)\n",
    "for i, chunk in enumerate(chunks):\n",
    "    print(f\"Chunk {i+1}:\\n{chunk}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "3d190746",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    }
   ],
   "source": [
    "summarizer = pipeline(\n",
    "    \"text2text-generation\",                     # tells HF the task type\n",
    "    model=\"Salesforce/codet5-base-multi-sum\",   # pretrained model for code summarization\n",
    "    tokenizer=\"Salesforce/codet5-base-multi-sum\"\n",
    ")\n",
    "\n",
    "\n",
    "# Run the summarization pipeline\n",
    "summary = summarizer(prompt, max_new_tokens=1000000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "289e0781",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extract and list all of the details from a module s abstract syntax tree .\n"
     ]
    }
   ],
   "source": [
    "print(summary[0]['generated_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "9a30c17b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('from pyspark.sql import SparkSession\\n\\ndef main():\\n    # Start Spark session\\n    spark = SparkSession.builder.appName(\"SQL_ETL_with_Variable\").getOrCreate()\\n\\n    # -----------------------------\\n    # 1. EXTRACT\\n    # -----------------------------\\n    source_table = \"source_db.people\"\\n    df = spark.sql(f\"SELECT id, name, age FROM {source_table}\")\\n    print(\"Original Data:\")\\n    df.show()\\n\\n    # -----------------------------\\n    # 2. TRANSFORM using SQL query\\n    # -----------------------------\\n    # Register temp view\\n    df.createOrReplaceTempView(\"people_view\")\\n\\n    # SQL query as a variable\\n    sql_query = \"\"\"\\n        SELECT \\n            id,\\n            name,\\n            age,\\n            CASE \\n                WHEN age > 30 THEN \\'senior\\'\\n                ELSE \\'junior\\'\\n            END AS status\\n        FROM people_view\\n        WHERE age IS NOT NULL\\n    \"\"\"\\n\\n    transformed_df = spark.sql(sql_query)\\n    print(\"Transformed Data:\")\\n    transformed_df.show()\\n\\n    # -----------------------------\\n    # 3. LOAD\\n    # -----------------------------\\n    target_table = \"target_db.people_status\"\\n    transformed_df.write.mode(\"overwrite\").saveAsTable(target_table)\\n\\n    print(f\"Data successfully written to {target_table}\")\\n\\n    spark.stop()\\n\\n\\nif __name__ == \"__main__\":\\n    main()', 'Extract and list the details from the provided columns .')\n",
      "\n",
      "=== Combined Summary ===\n",
      "\n",
      "Extract and list the details from the provided columns .\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "from transformers import pipeline\n",
    "import textwrap\n",
    "\n",
    "def extract_code_from_notebook(notebook_path):\n",
    "    \"\"\"Extracts and flattens all code from a Jupyter notebook into one string.\"\"\"\n",
    "    with open(notebook_path, 'r', encoding='utf-8') as f:\n",
    "        nb = json.load(f)\n",
    "\n",
    "    all_code = [\n",
    "        \"\".join(cell['source'])\n",
    "        for cell in nb.get('cells', [])\n",
    "        if cell['cell_type'] == 'code'\n",
    "    ]\n",
    "    return \"\\n\".join(all_code)\n",
    "\n",
    "def chunk_text(text, max_tokens=512):\n",
    "    \"\"\"Chunks text based on estimated token count.\"\"\"\n",
    "    lines = text.splitlines()\n",
    "    chunks, current_chunk = [], []\n",
    "    token_estimate = 0\n",
    "\n",
    "    for line in lines:\n",
    "        token_estimate += len(line.split())  # crude token estimate\n",
    "        current_chunk.append(line)\n",
    "\n",
    "        if token_estimate >= max_tokens:\n",
    "            chunks.append(\"\\n\".join(current_chunk))\n",
    "            current_chunk, token_estimate = [], 0\n",
    "\n",
    "    if current_chunk:\n",
    "        chunks.append(\"\\n\".join(current_chunk))\n",
    "\n",
    "    return chunks\n",
    "\n",
    "# --- Load and flatten notebook ---\n",
    "notebook_path = Path(r\"C:\\Users\\samle\\OneDrive\\Documents\\Utils\\Notebook_Test.ipynb\")\n",
    "flattened_code = extract_code_from_notebook(notebook_path)\n",
    "code_chunks = chunk_text(flattened_code, max_tokens=256)\n",
    "\n",
    "# --- Initialize summarization pipeline ---\n",
    "summarizer = pipeline(\n",
    "    \"text2text-generation\",\n",
    "    model=\"Salesforce/codet5-base-multi-sum\",\n",
    "    tokenizer=\"Salesforce/codet5-base-multi-sum\"\n",
    ")\n",
    "\n",
    "# --- Summarize each chunk ---\n",
    "summaries = []\n",
    "for i, chunk in enumerate(code_chunks):\n",
    "    prompt = textwrap.dedent(f\"\"\"\\\n",
    "        Extract and list the following details from the provided columns:\n",
    "        - Source Tables (table names)\n",
    "        - Source Columns (column names)\n",
    "        - Target Tables (table names)\n",
    "        - Target Columns (column names)\n",
    "        - Transformations (describe operations applied)\n",
    "\n",
    "        Code:\n",
    "        {chunk}\n",
    "    \"\"\")\n",
    "    result = summarizer(prompt, max_new_tokens=512)\n",
    "    print((chunk, result[0]['generated_text']))\n",
    "    summaries.append(result[0]['generated_text'])\n",
    "\n",
    "# --- Print combined result ---\n",
    "print(\"\\n=== Combined Summary ===\\n\")\n",
    "print(\"\\n\\n\".join(summaries))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3910fa78",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Summary for Cell 1 ---\n",
      "Extract and list the details from the provided code . This code extracts the data from the source_db . people table and transforms the data from the source_db . people_status table to the target_db . people_status table .\n",
      "\n",
      "--- Summary for Cell 2 ---\n",
      "Extract and list the details from the provided code .\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45b0da54",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
